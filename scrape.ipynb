{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIIE Coding Task\n",
    "*Author: Jiayan LI*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task involved scraping a list of image URLs and downloading the corresponding images from classmates.com. To accomplish this, I employed two main steps: automating the Chrome browser using `Selenium` with `undetected_chromedriver` and accessing image urls with `urllib.request`.\n",
    "\n",
    "My initial attempt using the `requests` library presented a 403 error, which indicates that the server is refusing access to the resource I'm trying to retrieve. Upon further investigation, I discovered that the website's terms of service prohibit automated access to their content using scripts and bots.\n",
    "\n",
    "In my second attempt using `Selenium`, I encountered several challenges. When using Chrome and Firefox drivers, a Cloudflare verification page appeared, indicating that the website is protected by Cloudflare and requires additional verification. When using Safari, the browser crashed repeatedly, preventing further progress.\n",
    "\n",
    "In an attempt to bypass the Cloudflare verification, I experimented with different solutions, and using an undetected ChromeDriver helped me successfully log in.\n",
    "\n",
    "For processing the image links, I initially encountered a 403 error when using the requests library. To overcome this, I found a solution that involved using an opener to download the images successfully.\n",
    "\n",
    "Overall, due to the website's protection mechanisms and terms of service, I faced challenges in both scraping and downloading images but ultimately solved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import undetected_chromedriver as uc\n",
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 & 2: Log in & Scrape one page\n",
    "Stored as `test.jpg`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `Selenium` to scrape the image urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using undetedcted chromedriver\n",
    "\n",
    "Ref: https://stackoverflow.com/questions/71518406/how-to-bypass-cloudflare-browser-checking-selenium-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a driver instance\n",
    "driver = uc.Chrome(use_subprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the webpage\n",
    "url = \"https://www.classmates.com/siteui/yearbooks/4182755124?page=7\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load completely\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-in page pops out at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the login form elements and fill in the credentials\n",
    "username_input = driver.find_element(By.NAME, \"emailOrRegId\")\n",
    "password_input = driver.find_element(By.NAME, \"password\")\n",
    "\n",
    "# Fill in credentials\n",
    "username = \"joannejiayan@gmail.com\"\n",
    "password = \"15070019719mama!\"\n",
    "\n",
    "username_input.send_keys(username)\n",
    "password_input.send_keys(password)\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scroll down the page, mimicing humna behaviors\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the login form\n",
    "submit_button = driver.find_element(By.ID, \"login-button\")\n",
    "driver.execute_script(\"arguments[0].click();\", submit_button)\n",
    "\n",
    "# Wait for the page to load after login\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 satisfied, log-in process completed and now I arrived at the [webpage](https://www.classmates.com/siteui/yearbooks/4182755124?page=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all image elements using XPath\n",
    "image_elements = driver.find_elements(By.XPATH, \"//img[contains(@src, 'https://yb.')]\")  # Example XPath\n",
    "\n",
    "# Extract the image URLs\n",
    "image_urls = [element.get_attribute(\"src\") for element in image_elements]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the test image url\n",
    "test_url = image_urls[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing image URL\n",
    "\n",
    "ref: https://stackoverflow.com/questions/34692009/download-image-from-url-using-python-urllib-but-receiving-http-error-403-forbid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test.jpg', <http.client.HTTPMessage at 0x7fd066222f10>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use opener\n",
    "opener=urllib.request.build_opener()\n",
    "opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# retrieve the image file\n",
    "local='test.jpg'        # specify the path\n",
    "urllib.request.urlretrieve(test_url, local)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 satisfied, `test.jpg` now in the directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Streamline the process\n",
    "Scrape all the pages of the yearbook (2010-Alameda-High-School). Results stored under the directory `images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list that contains every yearbook page URL\n",
    "yearbook_urls = []\n",
    "\n",
    "for i in range(1, 297):\n",
    "    page_url = \"https://www.classmates.com/siteui/yearbooks/4182755124?page=\" + f'{i}'\n",
    "    yearbook_urls.append(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.classmates.com/siteui/yearbooks/4182755124?page=296'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect if I've got every page url\n",
    "yearbook_urls[-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streamline the image-URL-scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_url(yearbook_page_url):\n",
    "    '''\n",
    "    Scrape the image url from specified page url\n",
    "    '''\n",
    "\n",
    "    driver.get(yearbook_page_url)\n",
    "\n",
    "    # Wait for the page to load completely\n",
    "    driver.implicitly_wait(5)\n",
    "\n",
    "    # if encounter a log-in page\n",
    "    try:\n",
    "        # Attempt to find the element\n",
    "        username_input = driver.find_element(By.NAME, \"emailOrRegId\")\n",
    "        # Element found\n",
    "        print(\"Log-in element exists\")\n",
    "\n",
    "        password_input = driver.find_element(By.NAME, \"password\")\n",
    "\n",
    "        # Fill in credentials\n",
    "        username = \"joannejiayan@gmail.com\"\n",
    "        password = \"15070019719mama!\"\n",
    "\n",
    "        username_input.send_keys(username)\n",
    "        password_input.send_keys(password)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Scroll down the page, mimicing humna behaviors\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Submit the login form\n",
    "        submit_button = driver.find_element(By.ID, \"login-button\")\n",
    "        driver.execute_script(\"arguments[0].click();\", submit_button)\n",
    "\n",
    "        # Wait for the page to load after login\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    except: pass\n",
    "\n",
    "    # Find all image elements using XPath\n",
    "    image_element = driver.find_element(By.XPATH, \"//img[contains(@src, 'https://yb.')]\")  # Example XPath\n",
    "\n",
    "    # Extract the image URLs\n",
    "    image_url = image_element.get_attribute(\"src\")\n",
    "\n",
    "    return image_url"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping image URLs in batches using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 completed\n",
      "Batch 2 completed\n",
      "Batch 3 completed\n",
      "Batch 4 completed\n",
      "Batch 5 completed\n",
      "Batch 6 completed\n",
      "Batch 7 completed\n",
      "Batch 8 completed\n",
      "Batch 9 completed\n",
      "Batch 10 completed\n",
      "Batch 11 completed\n",
      "Batch 12 completed\n",
      "Batch 13 completed\n",
      "Batch 14 completed\n",
      "Batch 15 completed\n",
      "Batch 16 completed\n",
      "Batch 17 completed\n",
      "Batch 18 completed\n",
      "Batch 19 completed\n",
      "Batch 20 completed\n",
      "Batch 21 completed\n",
      "Batch 22 completed\n",
      "Batch 23 completed\n",
      "Batch 24 completed\n",
      "Batch 25 completed\n",
      "Batch 26 completed\n",
      "Batch 27 completed\n",
      "Batch 28 completed\n",
      "Batch 29 completed\n",
      "Batch 30 completed\n"
     ]
    }
   ],
   "source": [
    "image_urls = []\n",
    "failed_attempts = []\n",
    "batch_size = 10  # Number of URLs to process in each batch\n",
    "\n",
    "# Calculate the total number of batches\n",
    "total_batches = len(yearbook_urls) // batch_size + (1 if len(yearbook_urls) % batch_size != 0 else 0)\n",
    "\n",
    "for batch in range(total_batches):\n",
    "    start_index = batch * batch_size\n",
    "    end_index = (batch + 1) * batch_size\n",
    "\n",
    "    for i, page_url in enumerate(yearbook_urls[start_index:end_index], start=start_index):\n",
    "        try:\n",
    "            image_urls.append(get_image_url(page_url))\n",
    "        except Exception as e:\n",
    "            failed_attempts.append(i)\n",
    "            print(f\"Attempt {i} failed: {str(e)}\")\n",
    "\n",
    "    print(f\"Batch {batch + 1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://yb.cmcdn.com/yearbooks/1/4/f/5/14f548096a10e39b9a984e8461acec86/440/0001.jpg?h=39f9265d252277f7817d0c50c274a54d',\n",
       " 'https://yb.cmcdn.com/yearbooks/1/4/f/5/14f548096a10e39b9a984e8461acec86/440/0002.jpg?h=0e35a7cb66c07162259002f6c8f4a734',\n",
       " 'https://yb.cmcdn.com/yearbooks/1/4/f/5/14f548096a10e39b9a984e8461acec86/440/0003.jpg?h=1ea75290fd69e34b0d17dc5d0f293674',\n",
       " 'https://yb.cmcdn.com/yearbooks/1/4/f/5/14f548096a10e39b9a984e8461acec86/440/0004.jpg?h=d157fdc61755da7ac709acf742013289',\n",
       " 'https://yb.cmcdn.com/yearbooks/1/4/f/5/14f548096a10e39b9a984e8461acec86/440/0005.jpg?h=9705548ecb80d5f4287c41a8fc60fdce']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_urls[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the image urls to `image_url.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image URLs have been written to the file: image_urls.txt\n"
     ]
    }
   ],
   "source": [
    "file_path = 'image_urls.txt'  # Path to the text file\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for url in image_urls:\n",
    "        file.write(url + '\\n')\n",
    "\n",
    "print(\"Image URLs have been written to the file:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download images from the collected image URLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory images\n",
    "directory = 'images'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(directory):\n",
    "    # Create the directory\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an opener\n",
    "opener=urllib.request.build_opener()\n",
    "opener.addheaders=[('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1941.0 Safari/537.36')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download each image from each url in the image url list\n",
    "for i, url in enumerate(image_urls):\n",
    "    local=f'images/{i}.jpg'\n",
    "    urllib.request.urlretrieve(url, local)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macs30112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
